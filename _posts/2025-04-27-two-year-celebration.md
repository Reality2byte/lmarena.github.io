---
layout: distill
title: "Celebrating Community Impact: 3M+ votes, 400+ models, and 300+ pre-release tests"
description:
giscus_comments: true
date: 2025-04-27
featured: true
thumbnail: assets/img/blog/arena_two_year/cover.png
authors:
  - name: LMArena Team
    affiliations:
      name: LMArena
---

To date, the community has evaluated over **400+ public models** on LMArena as well as **300+ pre-release tests**. **Tens of millions** of battle pairings have been served to users across the world, and each vote has shaped real-world AI performance and development. Around this time two years ago, the community helped us publish our [very first leaderboard rankings](https://lmsys.org/blog/2023-05-03-arena/):

<img src="/assets/img/blog/arena_two_year/first_leaderboard.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 95%">

It's amazing to see how far we’ve come together since then! Today we’re celebrating some fun figures outlining the scale of this community evaluation effort, showcasing how it has helped LLMs develop:
- 400+ models have been evaluated across various modalities (Text, [Vision](https://blog.lmarena.ai/blog/2024/multimodal/), [Text-to-Image](https://x.com/lmarena_ai/status/1882164189739073990), [WebDev](https://blog.lmarena.ai/blog/2025/webdev-arena/) and more!)
- 300+ evaluations have been pre-release (giving the community access to pre-release frontier models)
- 1.5 million prompts contributed by the community have been released for open research
- 9 papers and 15 blog posts on evaluation and human preference been published 

#### Current leaderboard (as of April 2025):
<img src="/assets/img/blog/arena_two_year/beta_leaderboard.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 95%">


### Model Releases on LMArena

<img src="/assets/img/blog/arena_two_year/leaderboard_breakdown.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 95%">


Our community wants to explore and test the frontier of AI. Guided by their interest, we track and release the best models as they come. Big, well-known labs like Google, Meta, Alibaba and OpenAI are represented, as they are incredibly active in releasing models the community is eager to test. At the same time, thanks to community support and requests, open-source and emerging labs have a platform here too. LMArena exists to serve the community’s curiosity and ability to explore all types of models. 

As part of our commitment to AI transparency, we’ve also released [public datasets](https://huggingface.co/lmarena-ai), and shared [evaluation methodology](https://blog.lmarena.ai/blog/) and [published papers](https://arxiv.org/abs/2403.04132) to help everyone in the ecosystem learn and build better AI. You can explore and learn alongside the community by cross-checking the model release numbers above. We also share 20% of this data back with model providers to help them improve their models from our community’s perspective. Check out more of our ongoing research on our [blog](https://blog.lmarena.ai/blog/).

### Sampling the Model Pairs

<img src="/assets/img/blog/arena_two_year/battle_breakdown.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 95%">

From the start, the LMArena experience has been community-driven. One trend that continues to come through in the feedback loud and clear is that a valuable experience involves having frequent access to the best models, and being a part of the first to access the newest ones. To meet the community expectations, our sampling [policy](https://blog.lmarena.ai/blog/2024/policy/) focuses on those two areas:
1. Preserve the community experience by serving the best models.
2. Make it easy to converge on new models as they arrive.

To achieve this, we upsample the best models, and new models, so they get seen more frequently in battles and provide the most value back to the community. The way we prioritize models is just one example of how community input shapes every aspect of the platform. 


### Supporting Open-Source

<img src="/assets/img/blog/arena_two_year/open_vs_proprietary.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 95%">

In addition to having ungated access to top models, the community remains especially excited and always passionate about testing the latest and greatest open-source ones. That said, the leaderboard is the place where the best models are revealed. Every model steps into the arena, but only the best rise to the top. Open source or proprietary, it doesn’t matter, the community calls the winners. 

We proudly support open-source models to be sampled into battle mode, with nearly 41% of battles involving an open model. Our community not only champions open-source initiatives but also understands firsthand how much more cost efficient and customizable these models can be for real-world applications. Open-source models and proprietary ones are given equal footing in receiving community feedback.


### Join the Community

We're going to keep working to make LMArena an even more inclusive and open experience. If you’ve been following our journey, you know we’re actively testing a [new experience in Beta](https://beta.lmarena.ai) to be more accessible to everyone. We’re also going to continue to expand the range of models we test, improving ways for new teams to participate, and building features to make the evaluation experience better for the whole ecosystem. If you're building something new or curious to explore the frontier of AI with others, we’d love for you to help shape the future of AI with us.

- Check out our new UI in Beta: [https://beta.lmarena.ai](https://beta.lmarena.ai)
- Join the conversation: [https://discord.gg/LMArena](https://discord.gg/LMArena)
- Follow us on X: [x.com/lmarena_ai](https://x.com/lmarena_ai) 